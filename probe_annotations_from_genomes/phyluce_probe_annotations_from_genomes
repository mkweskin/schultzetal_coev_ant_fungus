#!/usr/bin/env python2
# encoding: utf-8

"""
File: phyluce_probe_annotations_from_genomes
Original authored as 'phyluce_probe_slice_sequence_from_genomes' by Brant Faircloth
MODIFIED into 'phyluce_probe_annotations_from_genomes' with additions by Matthew Kweskin

Created by Brant Faircloth on 08 June 2013 11:06 PDT (-0700)
Copyright (c) 2013 Brant C. Faircloth. All rights reserved.

Description:
This is phyluce_probe_slice_sequence_from_genomes from Phyluce 1.6.7 modified
by Matthew Kweskin to extract gff annotations. It requries the gffutils python
library which can be added to your phyluce 1.6.7 (and earlier) conda install with:
conda install -c conda-forge -c bioconda --override-channels gffutils

NOTE:
This script is not compatible with Python3 and Phyluce 1.7.x
"""

import os
import re
import sys
import copy
import logging
import argparse
import ConfigParser
from collections import defaultdict
import gffutils

from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord

from bx.seq import twobit
from phyluce import lastz
from phyluce.helpers import FullPaths, CreateDir, is_dir, is_file


import pdb

def get_args():
    """Get arguments from CLI"""
    parser = argparse.ArgumentParser(
        description="""Given a LASTZ input directory, and gff file, find matches, add flank, and return a list of gff features that occur in the UCE""")
    parser.add_argument(
        "--conf",
        required=True,
        action=FullPaths,
        type=is_file,
        help="""Path to the configuration file. There should be a [scaffolds] section with the path to the 2bit files and a [gffs] section with paths to the gff files."""
    )
    parser.add_argument(
        "--lastz",
        required=True,
        action=FullPaths,
        type=is_dir,
        help="""Path to the directory containing LASTZ results"""
    )
    parser.add_argument(
        "--output",
        required=True,
        action=CreateDir,
        help="""Path to the output directory for storing report of features"""
    )
    parser.add_argument(
        "--name-pattern",
        dest="pattern",
        type=str,
        default=None,
        help="An alternate name pattern to transform the conf entry into"
    )
    parser.add_argument(
        '--probe-prefix',
        type=str,
        default="uce-",
        help='The prefix (e.g. "uce-") added to all probes designed'
    )
    parser.add_argument(
        '--probe-regex',
        type=str,
        default='^({}\d+)(?:_p\d+.*)',
        help='The regular expression to use for matching probes'
    )
    parser.add_argument(
        "--exclude",
        type=str,
        nargs='+',
        default=None,
        help="""Species to exclude from feature reporting (e.g. if there is no gff file)""",
    )
    parser.add_argument(
        "--verbosity",
        type=str,
        choices=["INFO", "WARN", "CRITICAL"],
        default="INFO",
        help="""The logging level to use"""
    )
    parser.add_argument(
        "--contig_orient",
        action="store_true",
        default=False,
        help="""Check orientation by contigs versus probes - useful for multi-species probe sets""",
    )
    flank = parser.add_mutually_exclusive_group(required=True)
    flank.add_argument(
        "--flank",
        type=int,
        default=500,
        help="""The amount of flanking sequence to add to each match""",
    )
    flank.add_argument(
        "--probes",
        type=int,
        default=None,
        help="""The probe length to use""",
    )
    return parser.parse_args()


def get_all_files_from_conf(conf, pattern=None):
    all_files = []
    if conf.has_section("chromos"):
        all_files.extend(conf.items("chromos"))
    if conf.has_section("scaffolds"):
        all_files.extend(conf.items("scaffolds"))
    if pattern is not None:
        files = [(v[0], pattern.format(v[0]), v[1]) for v in all_files]
    else:
        files = [(v[0], v[0], v[1]) for v in all_files]
    return files


def gffs_from_conf(conf):
    gffs = {}
    if conf.has_section("gffs"):
        for (name, value) in conf.items("gffs"):
            gffs[name]=value
    return gffs


def new_get_probe_name(header, regex):
    match = re.search(regex, header)
    return match.groups()[0]


def check_loci_for_dupes(matches):
    """Check for UCE loci that match more than one contig"""
    dupe_set = set([uce for uce, contigs in matches.iteritems() if len(contigs) > 1])
    return dupe_set


def slice_and_return_fasta(tb, name, min, max, flank, probes):
    if probes is None:
        if min - flank > 0:
            ss = min - flank
        else:
            ss = 0
        if max + flank < len(tb[name]):
            se = max + flank
        else:
            se = len(tb[name])
    else:
        length = abs(max - min)
        delta = probes - length
        if delta > 0:
            if delta % 2 != 0:
                delta += 1
            ss = min - delta / 2
            se = max + delta / 2
            if ss < 0:
                ss = 0
                se = se + (probes - se)
        else:
            ss = min
            se = max
    return ss, se, tb[name][ss:se]


def remove_ambiguous_ends(ss, se, sequence):
    start_matches = re.search("^([Nn]+)", sequence)
    end_matches = re.search("([Nn]+)$", sequence)
    if start_matches:
        new_start = len(start_matches.groups()[0])
        ss = ss + new_start
    else:
        new_start = 0
    if end_matches:
        new_end = len(end_matches.groups()[0])
        se = se - new_end
    else:
        new_end = 0
    return ss, se, sequence[new_start:len(sequence) - new_end]


def remove_repetitive_ends(ss, se, sequence):
    start_matches = re.search("^([acgt]+)", sequence)
    end_matches = re.search("([acgt]+)$", sequence)
    if start_matches:
        new_start = len(start_matches.groups()[0])
        ss = ss + new_start
    else:
        new_start = 0
    if end_matches:
        new_end = len(end_matches.groups()[0])
        se = se - new_end
    else:
        new_end = 0
    return ss, se, sequence[new_start:len(sequence) - new_end]


def build_sequence_object(cnt, contig, ss, se, uce, min, max, orient, sorted_positions, sequence, probes):
    # turn of the clipping if we're using these data for probes
    if not probes:
        ss, se, sequence = remove_ambiguous_ends(ss, se, sequence)
        ss, se, sequence = remove_repetitive_ends(ss, se, sequence)
        name_start = "Node_{0}_length_{1}_cov_1000".format(cnt, len(sequence))
    else:
        orient = list(orient)[0]
        if not orient == "+":
            orient = "revcomp"
        name_start = "slice_{}".format(cnt)
    name = "{0}|contig:{2}|slice:{3}-{4}|uce:{5}|match:{6}-{7}|orient:{8}|probes:{9}".format(
        name_start,
        len(sequence),
        contig,
        ss,
        se,
        uce,
        min,
        max,
        orient,
        len(sorted_positions)
    )
    if orient == "revcomp":
        return SeqRecord(Seq(sequence).reverse_complement(), id=name, name='', description='')
    else:
        return SeqRecord(Seq(sequence), id=name, name='', description='')


def parse_lastz_file(lz, contig_orient, regex):
    all_uce_names = set()
    uce_matches = defaultdict(lambda: defaultdict(list))
    orientation = defaultdict(lambda: defaultdict(set))
    for lz in lastz.Reader(lz, long_format=True):
        # get strandedness of match
        contig_name = lz.name1
        # get name of UCE from lastz info
        uce_name = new_get_probe_name(lz.name2, regex)
        # keep a record of all UCEs matched
        all_uce_names.add(uce_name)
        uce_matches[uce_name][contig_name].append([lz.zstart1, lz.end1])
        # usually, we get orientation matches by probes. but for mixed species
        # probes, they may be in several orientations, which can cause problems,
        # so check their orientation relative to the contig
        if contig_orient:
            orientation[uce_name][contig_name].add(lz.strand1)
        else:
            orientation[uce_name][contig_name].add(lz.strand2)
    return all_uce_names, uce_matches, orientation


def setup_logging(level):
    log = logging.getLogger("Phyluce")
    console = logging.StreamHandler(sys.stdout)
    if level == "INFO":
        log.setLevel(logging.INFO)
        console.setLevel(logging.INFO)
    if level == "WARN":
        log.setLevel(logging.WARN)
        console.setLevel(logging.WARN)
    if level == "CRITICAL":
        log.setLevel(logging.CRITICAL)
        console.setLevel(logging.CRITICAL)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console.setFormatter(formatter)
    log.addHandler(console)
    return log


def main():
    args = get_args()
    # setup logger
    log = setup_logging(args.verbosity)
    log.info("=================== Starting Phyluce: report annotations ===================")
    # parse config file of genome locations (genomes in 2bit format)
    conf = ConfigParser.ConfigParser()
    conf.optionxform = str
    conf.read(args.conf)
    # get the files associated with the config entries
    all_files = get_all_files_from_conf(conf, args.pattern)
    # get the gffs associated with the config entries (as a dictionary)
    gffs = gffs_from_conf(conf)

    for genome in all_files:
        short_name, long_name, twobit_name = genome
        text = " Working on {} genome ".format(short_name)
        log.info(text.center(65, "-"))
        if not args.exclude or (short_name not in args.exclude):
            out_name = os.path.join(args.output, "{}.gff".format(short_name.lower()))
            with open(out_name, 'w') as outf:
                log.info("Reading {} genome".format(short_name))
                tb = twobit.TwoBitFile(file(twobit_name))
                # parse the lastz results of the alignment
                lz = os.path.join(args.lastz, long_name)
                regex = args.probe_regex.format(args.probe_prefix)
                all_uce_names, uce_matches, orientation = parse_lastz_file(lz, args.contig_orient, regex)
                # we need to check nodes for dupe matches to the same probes
                uce_loci_matching_mult_contigs = check_loci_for_dupes(uce_matches)
                # delete those loci that hit multiple probes or had multiple probes hit them
                # this does not filter out contigs hitting multiple loci for the simple
                # reason that many loci will hit the same "contig" in circumstances where
                # the contig is large/chromosome sized.  These will get filtered out in
                # the next step of the matching process after we've created fasta sequences
                # representing each UCE locus
                for k in uce_matches.keys():
                    if k in uce_loci_matching_mult_contigs:
                        del uce_matches[k]
                # QC contig matches
                annot_found_count = 0
                annot_not_found_count = 0
                orient_drop = set()
                length_drop = set()
                # read gff into memory
                try:
                    gff = gffs[short_name]
                except KeyError:
                    print "ERROR: Name of gff file for " + short_name + " is missing from config file."
                    sys.exit()
                log.info("Reading gff: {}".format(gff))
                db = gffutils.create_db(gff, dbfn=':memory:', merge_strategy='merge')
                for uce_name, matches in uce_matches.iteritems():
                    bad = False

                    # make sure there is only one UCE match
                    assert len(matches.keys()) == 1, "There are multiple UCE matches"
                    for contig_name, positions in matches.iteritems():
                        # remove any probes with mixed orientation
                        orient = orientation[uce_name][contig_name]
                        if len(orient) > 1:
                            bad = True
                            orient_drop.add(uce_name)
                        if not bad:
                            # sort the positions for each contig
                            sorted_positions = sorted(positions)
                            if len(sorted_positions) > 1:
                                for i in range(1, len(sorted_positions)):
                                    # drop those contigs where probes fall > 500 bp apart
                                    if sorted_positions[i][0] - sorted_positions[i-1][1] > 500:
                                        bad = True
                                        length_drop.add(uce_name)
                                        break
                                    else:
                                        bad = False
                        if not bad:
                            min = sorted_positions[0][0]
                            max = sorted_positions[-1][-1]
                            # get the start/end (including flank), contig_name
                            ss, se, sequence = slice_and_return_fasta(tb, contig_name, min, max, args.flank, args.probes)

                            if args.probes and len(seq) < args.probes:
                                pass
                            else:
                                    annotfound = False
                                    location = contig_name + ":" + str(ss) + "-" + str(se)
                                    # check the sense strand for annotations
                                    annots = db.region(location, strand = "+")
                                    for annot in annots:
                                        annotfound = True
                                        outf.write(str(annot) + "; uce " + uce_name + "\n")
                                    # check the antisense strand for annotations
                                    annots = db.region(location, strand = "-")
                                    for annot in annots:
                                        annotfound = True
                                        outf.write(str(annot) + "; uce " + uce_name + "\n")
                                    if annotfound:
                                        annot_found_count += 1
                                    else:
                                        annot_not_found_count += 1
            output = "{}: {} uces, {} dupes, {} non-dupes, {} orient drop, {} length drop, {} annotations found, {} annotations not found".format(
                short_name,
                len(all_uce_names),
                len(uce_loci_matching_mult_contigs),
                len(uce_matches.keys()),
                len(orient_drop),
                len(length_drop),
                annot_found_count,
                annot_not_found_count
            )
            log.info(output)


if __name__ == '__main__':
    main()
